# 机器学习 

### 线性回归模型

1.线性回归模型
$$
h(x^{(i)}) = \omega x^{(i)} + b \approx y^{(i)}
$$
2.损失函数
$$
[ J(\omega,b) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 ]
$$
3.一元回归参数公式
$$
[ \omega = \frac{\sum_{i=1}^{m} (x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^{m} (x^{(i)} - \bar{x})^2} ]
$$
4.梯度下降
计算损失率(参数合适与否)
通过计算损失函数对每个参数的偏导数（即梯度），并沿着梯度的反方向更新参数，使得参数朝着使损失函数减小的方向移动。这样，经过多次迭代更新，我们期望能够在合理的学习率下逐渐接近损失函数的最小值，从而得到模型的最优参数。（即使斜率正负变化但都会使你更接近最小值
$$
[
\omega := \omega - \alpha \frac{\partial J(\omega, b)}{\partial \omega}
]
[
b := b - \alpha \frac{\partial J(\omega, b)}{\partial b}
]
$$

$$
[
\frac{\partial J(\omega, b)}{\partial \omega} = \frac{1}{m} \sum_{i=1}^{m} (\omega^T x^{(i)} - y^{(i)})x^{(i)}
]
$$





其中α代表学习率，即每次走下山谷的步长
**ps**:所有参数同时更新，到达最低点不移动的原因是偏导数为0



5.学习率α
过小太慢，过大反复横跳

6.多元线性回归
即是拥有多个特征值
利用行向量表示ω，列向量表示x，x.T.dot(ω) 或 np.dot(w, x.T)
**ps**：注意矩阵乘法的左右乘不同，x的上标表示第几行，下标表示第几列

*7.矢量化
利用矩阵乘法加快运算速度

8.多元梯度下降



9.特征收缩
当特征太大或太小，他们的参数都会相应的更加极端，导致梯度下降更加难以达到谷底，如特征太大，那么参数就会很小，导致他在山谷间来回横跳

+ 你可以让大的特征值每一个都除以他们中最大的值

+ Z分数归一化
	减去该特征的均值（计算该特征的均值）。
	将每个特征除以该特征的标准差（计算该特征的标准差）。
	$$
	[ X_{normalized} = \frac{X - \mu}{\sigma} ]
	[ \sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (X_i - \bar{X})^2} ]
	$$

- 平均归一化
	每个特征=（特征-均值）/（最大-最小）

10.绘制损失函数迭代曲线
x轴为迭代次数，Y轴为损失函数相应的值
图像应该保持单调递减，若存在增大，则意味着学习率α的太大了

11.多项式线性回归
将某一特征提升次数，对应的增加参数，改变损失函数

### 分类问题——逻辑回归

1.逻辑回归是一种用于处理分类问题的线性模型。尽管名称中带有"回归"，但逻辑回归实际上用于进行二元分类或多类别分类任务。它的基本原理是通过一个称为逻辑函数（logistic function）或Sigmoid函数的特殊函数，将输入特征的线性组合映射到0到1之间的概率值，表示样本属于正类（类别1）的概率。
$$
[ g(z) = \frac{1}{1 + e^{-z}} ]
$$

$$
[ z = \beta_0 + \beta_1X_1 + \beta_2X_2 + … + \beta_nX_n ]
$$



2.梯度下降
$$
[ \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} 1/2*(h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} ]
$$

$$
( \frac{1}{1 + e^{-\theta^Tx}} )
$$

3.先用散点图大致预测决策边界

4.成本函数（误差函数）
$$
[ J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))] ]
$$
![	](https://yilaoshi.oss-cn-guangzhou.aliyuncs.com/picture/image-20240320112532490.png)

5.梯度下降
$$
[ \omega := \omega - \alpha \nabla J(\omega) ]
$$
![image-20240320175212536](https://yilaoshi.oss-cn-guangzhou.aliyuncs.com/picture/image-20240320175212536.png)

### 正则化

1.原因：过拟合导致高方差，当模型具有很高的方差时，意味着模型对训练数据中的小变化非常敏感，表现为在不同的训练集上表现差异较大。

2.解决方法：
增加数据，减少特征，**正则化**

3.可以简单看做缩小特征的参数，一般不正则化b
可以让成本（损失）函数加上大权重\*参数，训练时为了让损失更小，就会让参数更小，接近0，通常会正则化所有特征 
$$
（Cost Function）= ( \frac{1}{2m} (\sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n} \theta_j^2) )
$$
4.梯度下降
$$
[
\frac{\partial J(\omega, b)}{\partial \omega} = \frac{1}{m} (\sum_{i=1}^{m} (\omega^T x^{(i)} - y^{(i)})x^{(i)} + \lambda\omega)
]
$$
.逻辑回归正则化
$$
[ J(\theta) = -\frac{1}{m} \left( \sum_{i=1}^{m} y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right) + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2 ]
$$

$$
[ \theta_j := \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j \right) ]
$$

5.L1正则化和L2正则化

- L1 范数（L1 norm）：向量中各个元素绝对值之和，表示为 ( ||x||_1 )。
	$$
	( ||x||*1 = \sum*{i=1}^{n} |x_i| )
	$$
	
- L2 范数（L2 norm）：向量中各个元素的平方和再开方，表示为 ( ||x||_2 )。
	$$
	( ||x||*2 = \sqrt{\sum*{i=1}^{n} x_i^2} )
	$$
	

正则化（Regularization）是在模型的训练过程中引入额外的信息（通常是模型参数的大小）来限制模型的复杂度，防止过拟合。在机器学习中，常见的正则化方式有 L1 正则化和 L2 正则化，通常会在损失函数中加入正则化项，使得优化目标不仅仅是降低训练误差，还包括对模型参数的惩罚。

与范数相关的正则化主要是 L1 范数正则化和 L2 范数正则化：

- L1 正则化会加入参数向量的 L1 范数作为惩罚项，使得模型更加稀疏，即倾向于将一些特征的权重缩减为零，从而实现特征选择的效果。
- L2 正则化会加入参数向量的 L2 范数的平方作为惩罚项，使得模型的参数更加平滑，防止参数值过大，降低模型复杂度，提高泛化能力。
